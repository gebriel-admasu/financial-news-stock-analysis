{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from textblob import TextBlob  # type: ignore\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the news dataset\n",
    "news_data = pd.read_csv(\"../data/raw_analyst_ratings.csv\")\n",
    "print(news_data.head())\n",
    "print(news_data.info())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyze headline length\n",
    "news_data['headline_length'] = news_data['headline'].str.len()\n",
    "sns.histplot(news_data['headline_length'], bins=30, kde=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.countplot(data=news_data, x='sentiment_category', palette='viridis')\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Number of Headlines')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Extract Common Keywords or Phrases\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=20)\n",
    "\n",
    "# Fit and transform headlines\n",
    "tfidf_matrix = tfidf.fit_transform(news_data['headline'])\n",
    "top_keywords = tfidf.get_feature_names_out()\n",
    "\n",
    "print(\"Top Keywords:\", top_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Initialize LDA model\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)  # 5 topics\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "# Display top words for each topic\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {idx + 1}:\")\n",
    "    print([tfidf.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result \n",
    "news_data.to_csv(\"../data/processed_sentiment_topic_modeling.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic modeling\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "yfinance_data_dir = '../data/yfinance_data/'\n",
    "csv_files = [f for f in os.listdir(yfinance_data_dir) if f.endswith('.csv')]\n",
    "stock_data = {file: pd.read_csv(os.path.join(yfinance_data_dir, file)) for file in csv_files}\n",
    "print(stock_data['AAPL_historical_data.csv'].head())  # Checking the data for AAPL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data['date'] = pd.to_datetime(news_data['date'], errors='coerce')\n",
    "print(news_data['date'].isnull().sum())  # Check for invalid entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_data['date'].dtypes)\n",
    "print(news_data['date'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data['publication_date'] = news_data['date'].dt.date\n",
    "daily_publication_counts = news_data['publication_date'].value_counts().sort_index()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(dpi= 200, figsize=(12, 6))\n",
    "daily_publication_counts.plot(kind='line', title=\"Daily Publication Trends\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publisher_counts = news_data['publisher'].value_counts()\n",
    "print(publisher_counts.head(10))  # Display the top 10 publishers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "top_publishers = publisher_counts.head(10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=top_publishers.index, y=top_publishers.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Top 10 Publishers by Article Count\")\n",
    "plt.xlabel(\"Publisher\")\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for publisher in top_publishers.index:\n",
    "    print(f\"\\nHeadlines for {publisher}:\")\n",
    "    print(news_data[news_data['publisher'] == publisher]['headline'].head(5))  # Show 5 sample headlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Analyze content for a specific publisher\n",
    "selected_publisher = top_publishers.index[0]  # Example: the most frequent publisher\n",
    "publisher_headlines = news_data[news_data['publisher'] == selected_publisher]['headline']\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=20)\n",
    "tfidf_matrix = tfidf.fit_transform(publisher_headlines)\n",
    "print(f\"Top Keywords for {selected_publisher}: {tfidf.get_feature_names_out()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data['publisher_domain'] = news_data['publisher'].apply(\n",
    "    lambda x: x.split('@')[-1] if '@' in x else None\n",
    ")\n",
    "domain_counts = news_data['publisher_domain'].value_counts()\n",
    "print(domain_counts.head(10))  # Display the top 10 domains\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_domains = domain_counts.head(10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=top_domains.index, y=top_domains.values, palette=\"coolwarm\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Top 10 Email Domains by Article Count\")\n",
    "plt.xlabel(\"Domain\")\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the yfinance_data folder\n",
    "data_folder = \"../data/yfinance_data\"\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(data_folder) if f.endswith('.csv')]\n",
    "\n",
    "# Load each file into a dictionary\n",
    "stock_data = {\n",
    "    file.split('_')[0]: pd.read_csv(os.path.join(data_folder, file)) for file in csv_files\n",
    "}\n",
    "\n",
    "# Display the first few rows of a loaded DataFrame (e.g., AAPL)\n",
    "print(stock_data['AAPL'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the data folder path\n",
    "data_folder = \"../data/yfinance_data\"\n",
    "\n",
    "# Initialize an empty dictionary to store stock DataFrames\n",
    "data = {}\n",
    "\n",
    "# Load all CSV files in the yfinance_data folder\n",
    "for file in os.listdir(data_folder):\n",
    "    if file.endswith(\"_historical_data.csv\"):\n",
    "        stock_symbol = file.split(\"_\")[0]  # Extract stock symbol from file name\n",
    "        file_path = os.path.join(data_folder, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['Ticker'] = stock_symbol  # Add a column for the stock symbol\n",
    "        data[stock_symbol] = df\n",
    "\n",
    "# Combine all data into one DataFrame\n",
    "stock_data = pd.concat(data.values(), ignore_index=True)\n",
    "print(stock_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talib\n",
    "\n",
    "# Apply technical indicators for each stock\n",
    "indicators = []\n",
    "for stock_symbol, df in data.items():\n",
    "    # Ensure 'Close' column is numeric\n",
    "    df['Close'] = pd.to_numeric(df['Close'], errors='coerce')\n",
    "    \n",
    "    # Simple Moving Average (SMA)\n",
    "    df['SMA_50'] = talib.SMA(df['Close'], timeperiod=50)\n",
    "    \n",
    "    # Relative Strength Index (RSI)\n",
    "    df['RSI_14'] = talib.RSI(df['Close'], timeperiod=14)\n",
    "    \n",
    "    # Moving Average Convergence Divergence (MACD)\n",
    "    df['MACD'], df['Signal_Line'], df['Histogram'] = talib.MACD(\n",
    "        df['Close'], fastperiod=12, slowperiod=26, signalperiod=9\n",
    "    )\n",
    "    \n",
    "    indicators.append(df)\n",
    "\n",
    "# Combine updated stock data\n",
    "stock_data_indicators = pd.concat(indicators, ignore_index=True)\n",
    "print(stock_data_indicators.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Date' is in datetime format\n",
    "aapl_data = data['AAPL']  # Replace 'AAPL' with the desired stock symbol\n",
    "aapl_data['Date'] = pd.to_datetime(aapl_data['Date'])  \n",
    "aapl_data['Date'] = pd.to_datetime(aapl_data['Date'])\n",
    "\n",
    "# Plot Closing Price and SMA with Date on x-axis\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(aapl_data['Date'], aapl_data['Close'], label='Close Price', color='blue')\n",
    "plt.plot(aapl_data['Date'], aapl_data['SMA_50'], label='50-Day SMA', color='red')\n",
    "plt.title('AAPL Closing Price and 50-Day SMA')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RSI with Date on x-axis\n",
    "aapl_data = data['AAPL']  # Replace 'AAPL' with the desired stock symbol\n",
    "aapl_data['Date'] = pd.to_datetime(aapl_data['Date'])  \n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(aapl_data['Date'], aapl_data['RSI_14'], label='RSI (14)', color='green')\n",
    "plt.axhline(70, color='red', linestyle='--', label='Overbought (70)')\n",
    "plt.axhline(30, color='blue', linestyle='--', label='Oversold (30)')\n",
    "plt.title('AAPL RSI Indicator')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('RSI')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import talib\n",
    "\n",
    "# Load data for a specific stock (e.g., AAPL)\n",
    "aapl_data = data['AAPL']  # Replace 'AAPL' with the desired stock symbol\n",
    "aapl_data['Date'] = pd.to_datetime(aapl_data['Date'])  # Ensure 'Date' is a datetime type\n",
    "\n",
    "# Calculate MACD, Signal Line, and Histogram\n",
    "aapl_data['MACD'], aapl_data['Signal_Line'], aapl_data['Histogram'] = talib.MACD(\n",
    "    aapl_data['Close'], fastperiod=12, slowperiod=26, signalperiod=9\n",
    ")\n",
    "\n",
    "# Plot the MACD, Signal Line, and Histogram\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot MACD and Signal Line\n",
    "plt.plot(aapl_data['Date'], aapl_data['MACD'], label='MACD', color='blue')\n",
    "plt.plot(aapl_data['Date'], aapl_data['Signal_Line'], label='Signal Line', color='red')\n",
    "\n",
    "# Plot Histogram as bars\n",
    "plt.bar(aapl_data['Date'], aapl_data['Histogram'], label='Histogram', color='gray', alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('MACD Indicator for AAPL')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check columns for a specific stock (e.g., AAPL)\n",
    "print(data['AAPL'].columns)\n",
    "\n",
    "# Check news dataset\n",
    "print(news_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Combine all stock DataFrames into one DataFrame\n",
    "combined_stock_data = pd.concat(data.values(), ignore_index=True)\n",
    "\n",
    "# Convert the 'Date' column to datetime format\n",
    "combined_stock_data['Date'] = pd.to_datetime(combined_stock_data['Date'])\n",
    "\n",
    "# Verify\n",
    "print(combined_stock_data[['Date', 'Ticker', 'Close']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date' column in news_data to datetime format\n",
    "news_data['date'] = pd.to_datetime(news_data['date'])\n",
    "\n",
    "# Verify\n",
    "print(news_data[['date', 'headline']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If 'news_data['date']' has time and you want to match only the date part, remove the time part\n",
    "news_data['date'] = news_data['date'].dt.date\n",
    "\n",
    "# Ensure both 'Date' columns are in the same format (datetime.date)\n",
    "combined_stock_data['Date'] = pd.to_datetime(combined_stock_data['Date']).dt.date\n",
    "\n",
    "# Merge datasets again\n",
    "aligned_data = pd.merge(\n",
    "    combined_stock_data, \n",
    "    news_data, \n",
    "    left_on='Date', \n",
    "    right_on='date', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Check the merged data\n",
    "print(aligned_data[['Date', 'Close', 'headline']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming stock_data and news_data are already loaded\n",
    "\n",
    "# Convert dates to datetime format\n",
    "combined_stock_data['Date'] = pd.to_datetime(combined_stock_data['Date'])\n",
    "news_data['date'] = pd.to_datetime(news_data['date'])\n",
    "\n",
    "# Merge datasets on the 'Date' column (or 'date' in the news dataset)\n",
    "merged_data = pd.merge(combined_stock_data, news_data, left_on='Date', right_on='date', how='inner')\n",
    "\n",
    "# Check the merged data\n",
    "print(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Function to calculate sentiment polarity\n",
    "def get_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Apply sentiment analysis to the 'headline' column\n",
    "merged_data['sentiment'] = merged_data['headline'].apply(get_sentiment)\n",
    "\n",
    "# Show the data with sentiment scores\n",
    "print(merged_data[['headline', 'sentiment']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily stock returns (percentage change in closing prices)\n",
    "merged_data['daily_return'] = merged_data['Close'].pct_change() * 100  # Percentage change\n",
    "print(merged_data[['Date', 'Close', 'daily_return']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate Sentiments\n",
    "# Group by Date and calculate average sentiment for each day\n",
    "daily_sentiment = merged_data.groupby('Date')['sentiment'].mean().reset_index()\n",
    "\n",
    "# Merge average sentiment back into the main data (to calculate correlation later)\n",
    "merged_data = pd.merge(merged_data, daily_sentiment, on='Date', suffixes=('', '_avg'))\n",
    "\n",
    "print(merged_data[['Date', 'sentiment', 'sentiment_avg']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate correlation\n",
    "# Calculate the Pearson correlation coefficient between average sentiment and daily returns\n",
    "correlation = merged_data[['sentiment_avg', 'daily_return']].corr().iloc[0, 1]\n",
    "print(f\"Pearson Correlation: {correlation}\")\n",
    "#A positive correlation suggests that positive sentiment correlates with positive stock returns, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot sentiment vs daily stock return\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(merged_data['sentiment_avg'], merged_data['daily_return'], alpha=0.5)\n",
    "plt.title('Sentiment vs Daily Stock Returns')\n",
    "plt.xlabel('Average Sentiment')\n",
    "plt.ylabel('Daily Stock Return (%)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
