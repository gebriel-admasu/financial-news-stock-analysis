{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/gabie/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from textblob import TextBlob  # type: ignore\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                           headline  \\\n",
      "0           0            Stocks That Hit 52-Week Highs On Friday   \n",
      "1           1         Stocks That Hit 52-Week Highs On Wednesday   \n",
      "2           2                      71 Biggest Movers From Friday   \n",
      "3           3       46 Stocks Moving In Friday's Mid-Day Session   \n",
      "4           4  B of A Securities Maintains Neutral on Agilent...   \n",
      "\n",
      "                                                 url          publisher  \\\n",
      "0  https://www.benzinga.com/news/20/06/16190091/s...  Benzinga Insights   \n",
      "1  https://www.benzinga.com/news/20/06/16170189/s...  Benzinga Insights   \n",
      "2  https://www.benzinga.com/news/20/05/16103463/7...         Lisa Levin   \n",
      "3  https://www.benzinga.com/news/20/05/16095921/4...         Lisa Levin   \n",
      "4  https://www.benzinga.com/news/20/05/16095304/b...         Vick Meyer   \n",
      "\n",
      "                        date stock  \n",
      "0  2020-06-05 10:30:54-04:00     A  \n",
      "1  2020-06-03 10:45:20-04:00     A  \n",
      "2  2020-05-26 04:30:07-04:00     A  \n",
      "3  2020-05-22 12:45:06-04:00     A  \n",
      "4  2020-05-22 11:38:59-04:00     A  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1407328 entries, 0 to 1407327\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count    Dtype \n",
      "---  ------      --------------    ----- \n",
      " 0   Unnamed: 0  1407328 non-null  int64 \n",
      " 1   headline    1407328 non-null  object\n",
      " 2   url         1407328 non-null  object\n",
      " 3   publisher   1407328 non-null  object\n",
      " 4   date        1407328 non-null  object\n",
      " 5   stock       1407328 non-null  object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 64.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load the news dataset\n",
    "news_data = pd.read_csv(\"../data/raw_analyst_ratings.csv\")\n",
    "print(news_data.head())\n",
    "print(news_data.info())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyze headline length\n",
    "news_data['headline_length'] = news_data['headline'].str.len()\n",
    "sns.histplot(news_data['headline_length'], bins=30, kde=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the sentiment analyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Calculate sentiment scores\n",
    "news_data['sentiment'] = news_data['headline'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "# Categorize sentiment as positive, neutral, or negative\n",
    "news_data['sentiment_category'] = news_data['sentiment'].apply(\n",
    "    lambda score: 'positive' if score > 0.05 else ('negative' if score < -0.05 else 'neutral')\n",
    ")\n",
    "\n",
    "# Display sentiment distribution\n",
    "print(news_data['sentiment_category'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.countplot(data=news_data, x='sentiment_category', palette='viridis')\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Number of Headlines')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Extract Common Keywords or Phrases\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=20)\n",
    "\n",
    "# Fit and transform headlines\n",
    "tfidf_matrix = tfidf.fit_transform(news_data['headline'])\n",
    "top_keywords = tfidf.get_feature_names_out()\n",
    "\n",
    "print(\"Top Keywords:\", top_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Initialize LDA model\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)  # 5 topics\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "# Display top words for each topic\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {idx + 1}:\")\n",
    "    print([tfidf.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result \n",
    "news_data.to_csv(\"../data/processed_sentiment_topic_modeling.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date      Open      High       Low     Close  Adj Close     Volume  \\\n",
      "0  1980-12-12  0.128348  0.128906  0.128348  0.128348   0.098943  469033600   \n",
      "1  1980-12-15  0.122210  0.122210  0.121652  0.121652   0.093781  175884800   \n",
      "2  1980-12-16  0.113281  0.113281  0.112723  0.112723   0.086898  105728000   \n",
      "3  1980-12-17  0.115513  0.116071  0.115513  0.115513   0.089049   86441600   \n",
      "4  1980-12-18  0.118862  0.119420  0.118862  0.118862   0.091630   73449600   \n",
      "\n",
      "   Dividends  Stock Splits  \n",
      "0        0.0           0.0  \n",
      "1        0.0           0.0  \n",
      "2        0.0           0.0  \n",
      "3        0.0           0.0  \n",
      "4        0.0           0.0  \n"
     ]
    }
   ],
   "source": [
    "# topic modeling\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "yfinance_data_dir = '../data/yfinance_data/'\n",
    "csv_files = [f for f in os.listdir(yfinance_data_dir) if f.endswith('.csv')]\n",
    "stock_data = {file: pd.read_csv(os.path.join(yfinance_data_dir, file)) for file in csv_files}\n",
    "print(stock_data['AAPL_historical_data.csv'].head())  # Checking the data for AAPL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1351341\n"
     ]
    }
   ],
   "source": [
    "news_data['date'] = pd.to_datetime(news_data['date'], errors='coerce')\n",
    "print(news_data['date'].isnull().sum())  # Check for invalid entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_data['date'].dtypes)\n",
    "print(news_data['date'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data['publication_date'] = news_data['date'].dt.date\n",
    "daily_publication_counts = news_data['publication_date'].value_counts().sort_index()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(dpi= 200, figsize=(12, 6))\n",
    "daily_publication_counts.plot(kind='line', title=\"Daily Publication Trends\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publisher_counts = news_data['publisher'].value_counts()\n",
    "print(publisher_counts.head(10))  # Display the top 10 publishers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "top_publishers = publisher_counts.head(10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=top_publishers.index, y=top_publishers.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Top 10 Publishers by Article Count\")\n",
    "plt.xlabel(\"Publisher\")\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for publisher in top_publishers.index:\n",
    "    print(f\"\\nHeadlines for {publisher}:\")\n",
    "    print(news_data[news_data['publisher'] == publisher]['headline'].head(5))  # Show 5 sample headlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Analyze content for a specific publisher\n",
    "selected_publisher = top_publishers.index[0]  # Example: the most frequent publisher\n",
    "publisher_headlines = news_data[news_data['publisher'] == selected_publisher]['headline']\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=20)\n",
    "tfidf_matrix = tfidf.fit_transform(publisher_headlines)\n",
    "print(f\"Top Keywords for {selected_publisher}: {tfidf.get_feature_names_out()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data['publisher_domain'] = news_data['publisher'].apply(\n",
    "    lambda x: x.split('@')[-1] if '@' in x else None\n",
    ")\n",
    "domain_counts = news_data['publisher_domain'].value_counts()\n",
    "print(domain_counts.head(10))  # Display the top 10 domains\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_domains = domain_counts.head(10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=top_domains.index, y=top_domains.values, palette=\"coolwarm\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Top 10 Email Domains by Article Count\")\n",
    "plt.xlabel(\"Domain\")\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date      Open      High       Low     Close  Adj Close     Volume  \\\n",
      "0  1980-12-12  0.128348  0.128906  0.128348  0.128348   0.098943  469033600   \n",
      "1  1980-12-15  0.122210  0.122210  0.121652  0.121652   0.093781  175884800   \n",
      "2  1980-12-16  0.113281  0.113281  0.112723  0.112723   0.086898  105728000   \n",
      "3  1980-12-17  0.115513  0.116071  0.115513  0.115513   0.089049   86441600   \n",
      "4  1980-12-18  0.118862  0.119420  0.118862  0.118862   0.091630   73449600   \n",
      "\n",
      "   Dividends  Stock Splits  \n",
      "0        0.0           0.0  \n",
      "1        0.0           0.0  \n",
      "2        0.0           0.0  \n",
      "3        0.0           0.0  \n",
      "4        0.0           0.0  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the yfinance_data folder\n",
    "data_folder = \"../data/yfinance_data\"\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [f for f in os.listdir(data_folder) if f.endswith('.csv')]\n",
    "\n",
    "# Load each file into a dictionary\n",
    "stock_data = {\n",
    "    file.split('_')[0]: pd.read_csv(os.path.join(data_folder, file)) for file in csv_files\n",
    "}\n",
    "\n",
    "# Display the first few rows of a loaded DataFrame (e.g., AAPL)\n",
    "print(stock_data['AAPL'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSLA_historical_data.csv columns: Index(['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume',\n",
      "       'Dividends', 'Stock Splits'],\n",
      "      dtype='object')\n",
      "AAPL_historical_data.csv columns: Index(['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume',\n",
      "       'Dividends', 'Stock Splits'],\n",
      "      dtype='object')\n",
      "MSFT_historical_data.csv columns: Index(['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume',\n",
      "       'Dividends', 'Stock Splits'],\n",
      "      dtype='object')\n",
      "GOOG_historical_data.csv columns: Index(['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume',\n",
      "       'Dividends', 'Stock Splits'],\n",
      "      dtype='object')\n",
      "NVDA_historical_data.csv columns: Index(['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume',\n",
      "       'Dividends', 'Stock Splits'],\n",
      "      dtype='object')\n",
      "AMZN_historical_data.csv columns: Index(['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume',\n",
      "       'Dividends', 'Stock Splits'],\n",
      "      dtype='object')\n",
      "META_historical_data.csv columns: Index(['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume',\n",
      "       'Dividends', 'Stock Splits'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for symbol, df in stock_data.items():\n",
    "    print(f\"{symbol} columns: {df.columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol, df in stock_data.items():\n",
    "    df.columns = df.columns.str.strip()  # Strip whitespace from column names\n",
    "    df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]  # Keep only relevant columns\n",
    "    stock_data[symbol] = df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
